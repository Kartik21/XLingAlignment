{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset #load_dataset from Huggingface\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata, spearmanr, pearsonr\n",
    "import statsmodels.stats.proportion as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.format\"] = 'pdf'\n",
    "plt.rcParams['font.family'] = 'Palatino'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_DICT = {'afrikaans':'afr_Latn' ,\n",
    "'english': 'eng_Latn',\n",
    "'amharic':'amh_Ethi' ,\n",
    "'armenian':'hye_Armn' ,\n",
    "'assamese':'asm_Beng' ,\n",
    "'basque':'eus_Latn' ,\n",
    "'bengali':'ben_Beng' ,\n",
    "'bulgarian':'bul_Cyrl' ,\n",
    "'burmese':'mya_Mymr' ,\n",
    "'catalan':'cat_Latn' ,\n",
    "'central kurdish':'ckb_Arab' ,\n",
    "'croatian': 'hrv_Latn',\n",
    "'dutch': 'nld_Latn',\n",
    "'xhosa': 'xho_Latn',\n",
    "'macedonian': 'mkd_Cyrl',\n",
    "'czech':'ces_Latn' ,\n",
    "'danish':'dan_Latn' ,\n",
    "'eastern panjabi':'pan_Guru' ,\n",
    "'egyptian arabic':'arz_Arab' ,\n",
    "'estonian':'est_Latn' ,\n",
    "'finnish':'fin_Latn' ,\n",
    "'french':'fra_Latn' ,\n",
    "'georgian':'kat_Geor' ,\n",
    "'german':'deu_Latn' ,\n",
    "'greek':'ell_Grek' ,\n",
    "'gujarati':'guj_Gujr' ,\n",
    "'hausa':'hau_Latn' ,\n",
    "'hebrew':'heb_Hebr' ,\n",
    "'hindi':'hin_Deva' ,\n",
    "'hungarian':'hun_Latn' ,\n",
    "'icelandic':'isl_Latn' ,\n",
    "'indonesian':'ind_Latn' ,\n",
    "'italian':'ita_Latn' ,\n",
    "'japanese':'jpn_Jpan' ,\n",
    "'javanese':'jav_Latn' ,\n",
    "'kannada':'kan_Knda' ,\n",
    "'kazakh':'kaz_Cyrl' ,\n",
    "'khmer':'khm_Khmr' ,\n",
    "'korean':'kor_Hang' ,\n",
    "'kyrgyz':'kir_Cyrl' ,\n",
    "'lao':'lao_Laoo' ,\n",
    "'lithuanian':'lit_Latn' ,\n",
    "'malayalam':'mal_Mlym' ,\n",
    "'marathi':'mar_Deva' ,\n",
    "'mesopotamian arabic':'acm_Arab' ,\n",
    "'modern standard arabic':'arb_Arab' ,\n",
    "'moroccan arabic':'ary_arab' ,\n",
    "'najdi arabic':'ars_Arab' ,\n",
    "'nepali':'npi_Deva' ,\n",
    "'north azerbaijani':'azj_Latn' ,\n",
    "'north levantine arabic':'apc_Arab' ,\n",
    "'northern uzbek':'uzn_Latn' ,\n",
    "'norwegian bokmal':'nob_Latn' ,\n",
    "'odia':'ory_Orya' ,\n",
    "'polish':'pol_Latn' ,\n",
    "'portuguese':'por_Latn' ,\n",
    "'romanian':'ron_Latn' ,\n",
    "'russian':'rus_Cyrl' ,\n",
    "'serbian':'srp_Cyrl' ,\n",
    "'simplified chinese':'zho_Hans' ,\n",
    "'sindhi':'snd_Arab' ,\n",
    "'sinhala':'sin_Sinh' ,\n",
    "'slovak':'slk_Latn' ,\n",
    "'slovenian':'slv_Latn' ,\n",
    "'somali':'som_Latn' ,\n",
    "'southern pashto':'pbt_Arab' ,\n",
    "'spanish':'spa_Latn' ,\n",
    "'standard latvian':'lvs_Latn' ,\n",
    "'standard malay':'zsm_Latn' ,\n",
    "'sundanese':'sun_Latn' ,\n",
    "'swahili':'swh_Latn' ,\n",
    "'swedish':'swe_Latn' ,\n",
    "'tamil':'tam_Taml' ,\n",
    "'telugu':'tel_Telu' ,\n",
    "'thai':'tha_Thai' ,\n",
    "'tosk albanian':'als_Latn' ,\n",
    "'traditional chinese':'zho_Hant' ,\n",
    "'turkish':'tur_Latn' ,\n",
    "'ukrainian':'ukr_Cyrl' ,\n",
    "'urdu':'urd_Arab' ,\n",
    "'vietnamese':'vie_Latn' ,\n",
    "'western persian':'pes_Arab'}\n",
    "\n",
    "LANGUAGE=[k for k,v in LANG_DICT.items()]\n",
    "LANGUAGE_wo_ENGLISH = [k for k,v in LANG_DICT.items() if k!='english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amharic', 'armenian', 'assamese', 'burmese', 'central kurdish', 'eastern panjabi', 'egyptian arabic', 'georgian', 'gujarati', 'hausa', 'javanese', 'kannada', 'khmer', 'kyrgyz', 'lao', 'malayalam', 'marathi', 'mesopotamian arabic', 'najdi arabic', 'nepali', 'north azerbaijani', 'north levantine arabic', 'northern uzbek', 'norwegian bokmal', 'odia', 'serbian', 'sindhi', 'sinhala', 'somali', 'southern pashto', 'sundanese', 'tamil', 'telugu', 'urdu']\n"
     ]
    }
   ],
   "source": [
    "LR_LANG = ['acm_Arab',\n",
    "'amh_Ethi',\n",
    "'apc_Arab',\n",
    "'ars_Arab',\n",
    "'ary_Arab',\n",
    "'arz_Arab',\n",
    "'asm_Beng',\n",
    "'azj_Latn',\n",
    "'ckb_Arab',\n",
    "'guj_Gujr',\n",
    "'hau_Latn',\n",
    "'hye_Armn',\n",
    "'jav_Latn',\n",
    "'kan_Knda',\n",
    "'kat_Geor',\n",
    "'khm_Khmr',\n",
    "'kir_Cyrl',\n",
    "'lao_Laoo',\n",
    "'mal_Mlym',\n",
    "'mar_Deva',\n",
    "'mya_Mymr',\n",
    "'nob_Latn',\n",
    "'npi_Deva',\n",
    "'ory_Orya',\n",
    "'pan_Guru',\n",
    "'pbt_Arab',\n",
    "'sin_Sinh',\n",
    "'snd_Arab',\n",
    "'som_Latn',\n",
    "'srp_Cyrl',\n",
    "'sun_Latn',\n",
    "'tam_Taml',\n",
    "'tel_Telu',\n",
    "'urd_Arab',\n",
    "'uzn_Latn']\n",
    "\n",
    "print([k for k,v in LANG_DICT.items() if v in LR_LANG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_LANG = ['afr_Latn',\n",
    "'als_Latn',\n",
    "'arb_Arab',\n",
    "'ben_Beng',\n",
    "'bul_Cyrl',\n",
    "'cat_Latn',\n",
    "'ces_Latn',\n",
    "'dan_Latn',\n",
    "'deu_Latn',\n",
    "'ell_Grek',\n",
    "'est_Latn',\n",
    "'eus_Latn',\n",
    "'fin_Latn',\n",
    "'fra_Latn',\n",
    "'heb_Hebr',\n",
    "'hin_Deva',\n",
    "'hrv_Latn',\n",
    "'hun_Latn',\n",
    "'ind_Latn',\n",
    "'isl_Latn',\n",
    "'ita_Latn',\n",
    "'jpn_Jpan',\n",
    "'kaz_Cyrl',\n",
    "'kor_Hang',\n",
    "'lit_Latn',\n",
    "'lvs_Latn',\n",
    "'mkd_Cyrl',\n",
    "'nld_Latn',\n",
    "'pes_Arab',\n",
    "'pol_Latn',\n",
    "'por_Latn',\n",
    "'ron_Latn',\n",
    "'rus_Cyrl',\n",
    "'slk_Latn',\n",
    "'slv_Latn',\n",
    "'spa_Latn',\n",
    "'swe_Latn',\n",
    "'swh_Latn',\n",
    "'tha_Thai',\n",
    "'tur_Latn',\n",
    "'ukr_Cyrl',\n",
    "'vie_Latn',\n",
    "'xho_Latn',\n",
    "'zho_Hans',\n",
    "'zho_Hant',\n",
    "'zsm_Latn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DALI(dataset, lang, model, mode):\n",
    "    \n",
    "    if mode == 'DALI':\n",
    "        lang_code = LANG_DICT[lang]\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali/DALI_{lang_code}_lasttoken.json'\n",
    "    if mode == 'DALIStrong':\n",
    "        lang_code = LANG_DICT[lang]\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali_strong/DALI_{lang_code}_lasttoken.json'\n",
    "\n",
    "    if mode == 'MEXAFlores':\n",
    "        lang_code = LANG_DICT[lang]\n",
    "        DAS_path = f'../../alignment_outputs/{model}/flores_mexa/{lang_code}.json'\n",
    "    if mode == 'MEXATask':\n",
    "        if dataset=='belebele' or dataset == 'flores':\n",
    "            lang_code = LANG_DICT[lang]\n",
    "            DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang_code}.json'\n",
    "        else:\n",
    "            DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang}.json'\n",
    "    with open(DAS_path) as f:\n",
    "        lang_DAS = json.load(f)\n",
    "    return lang_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_agg(dataset, model, field='flores_passage'):\n",
    "    if dataset == 'flores':\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}_100/sentence/entoxx_{model}_{dataset}_COMET.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}_100/sentence/xxtoen_{model}_{dataset}_COMET.json'\n",
    "        #list_of_languages = ['modern standard arabic', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'simplified chinese', 'swahili']\n",
    "        #lang_key = {'modern standard arabic': 'arabic', 'spanish': 'spanish', 'basque': 'basque', 'hindi': 'hindi', 'indonesian': 'indonesian', 'burmese': 'burmese', 'russian': 'russian', 'telugu': 'telugu', 'simplified chinese': 'chinese', 'swahili': 'swahili'}\n",
    "\n",
    "        with open(entoxxpath) as f:\n",
    "            entoxx_COMET = json.load(f)\n",
    "    \n",
    "        with open(xxtoenpath) as f:\n",
    "            xxtoen_COMET = json.load(f)\n",
    "        \n",
    "        entoxx_COMET_filtered = {}\n",
    "        xxtoen_COMET_filtered = {}\n",
    "        for k,v in entoxx_COMET.items():\n",
    "            if k in LANG_DICT.keys():\n",
    "                entoxx_COMET_filtered[k] = v[0]\n",
    "        for k,v in xxtoen_COMET.items():\n",
    "            if k in LANG_DICT.keys():\n",
    "                xxtoen_COMET_filtered[k] = v[0]\n",
    "\n",
    "    if dataset == 'belebele':\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}/{field}/entoxx_{model}_{dataset}_COMET.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}/{field}/xxtoen_{model}_{dataset}_COMET.json'\n",
    "\n",
    "        with open(entoxxpath) as f:\n",
    "            entoxx_COMET = json.load(f)\n",
    "        with open(xxtoenpath) as f:\n",
    "            xxtoen_COMET = json.load(f)\n",
    "        entoxx_COMET_filtered = {}\n",
    "        xxtoen_COMET_filtered = {}\n",
    "\n",
    "        for k,v in entoxx_COMET.items():\n",
    "            entoxx_COMET_filtered[k] = v[0]\n",
    "        for k,v in xxtoen_COMET.items():\n",
    "            xxtoen_COMET_filtered[k] = v[0]           \n",
    "    return entoxx_COMET_filtered, xxtoen_COMET_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_sample(dataset, model, field):\n",
    "\n",
    "    if dataset=='flores':\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}_100/{field}/entoxx_{model}_{dataset}_COMET_sample.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}_100/{field}/xxtoen_{model}_{dataset}_COMET_sample.json'\n",
    "    else:\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}/{field}/entoxx_{model}_{dataset}_COMET_sample.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}/{field}/xxtoen_{model}_{dataset}_COMET_sample.json'\n",
    "\n",
    "\n",
    "    with open(entoxxpath) as f:\n",
    "        entoxx_COMET_sample = json.load(f)\n",
    "    with open(xxtoenpath) as f:\n",
    "        xxtoen_COMET_sample = json.load(f)\n",
    "\n",
    "    return entoxx_COMET_sample, xxtoen_COMET_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_level_translation(entoxx_sample, xxtoen_sample, dataset):\n",
    "\n",
    "    LANGUAGE_DICT = {'xstorycloze': ['arabic', 'chinese', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'swahili'],\n",
    "                     'xcopa': ['chinese', 'indonesian', 'italian', 'swahili', 'tamil', 'thai', 'turkish', 'vietnamese'],\n",
    "                     'belebele': LANGUAGE_wo_ENGLISH,\n",
    "                     'flores': LANGUAGE_wo_ENGLISH}\n",
    "    \n",
    "    selected_lang_list = LANGUAGE_DICT[dataset]\n",
    "\n",
    "    entoxxdelta_results = defaultdict(dict)\n",
    "    xxtoendelta_results = defaultdict(dict)\n",
    "\n",
    "    if dataset=='flores':\n",
    "        n=100\n",
    "    if dataset == 'belebele':\n",
    "        n=900\n",
    "    if dataset == 'xstorycloze':\n",
    "        n=1511\n",
    "    if dataset == 'xcopa':\n",
    "        n=500\n",
    "\n",
    "\n",
    "    for lang in selected_lang_list:\n",
    "        if dataset =='flores':\n",
    "            lang_DAS = plot_DALI(dataset, lang, 'Llama3.1', 'MEXAFlores')\n",
    "        else:\n",
    "            lang_DAS = plot_DALI(dataset, lang, 'Llama3.1', 'MEXATask')\n",
    "\n",
    "        lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "        lang_DAS_formatted = defaultdict(dict)\n",
    "        for layer in range(32):\n",
    "            for i,dali in enumerate(lang_DAS[layer]):\n",
    "                lang_DAS_formatted[i][layer] = lang_DAS[layer][i]\n",
    "\n",
    "        all_list = defaultdict(list)\n",
    "        for item in range(n):\n",
    "            for layer in range(32):\n",
    "                all_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "\n",
    "        all_mean = []\n",
    "\n",
    "        for k,v in all_list.items():\n",
    "            all_mean.append(np.mean(v))\n",
    "\n",
    "        alignment_in_max_layer = lang_DAS[np.argmax(all_mean)]\n",
    "\n",
    "        if sum(lang_DAS[np.argmax(all_mean)]) == len(lang_DAS[np.argmax(all_mean)]):\n",
    "            entoxxdelta_results[lang]['delta'] = 'NA'\n",
    "            entoxxdelta_results[lang]['Utest_pval'] = 'NA'\n",
    "            entoxxdelta_results[lang]['ttest_pval'] = 'NA'\n",
    "            entoxxdelta_results[lang]['N_aligned'] = len(lang_DAS[np.argmax(all_mean)])\n",
    "            entoxxdelta_results[lang]['N_nonaligned'] = 0\n",
    "\n",
    "\n",
    "            xxtoendelta_results[lang]['delta'] = 'NA'\n",
    "            xxtoendelta_results[lang]['Utest_pval'] = 'NA'\n",
    "            xxtoendelta_results[lang]['ttest_pval'] = 'NA'\n",
    "            xxtoendelta_results[lang]['N_aligned'] = len(lang_DAS[np.argmax(all_mean)])\n",
    "            xxtoendelta_results[lang]['N_nonaligned'] = 0\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            alignment_zero_idx = [i for i,mexa in enumerate(alignment_in_max_layer) if mexa==0]\n",
    "            alignment_one_idx = [i for i,mexa in enumerate(alignment_in_max_layer) if mexa==1]\n",
    "\n",
    "            aligned_entoxx_scores = [entoxx_sample[lang]['scores'][i] for i in alignment_one_idx]\n",
    "            aligned_xxtoen_scores = [xxtoen_sample[lang]['scores'][i] for i in alignment_one_idx]\n",
    "\n",
    "            misaligned_entoxx_scores = [entoxx_sample[lang]['scores'][i] for i in alignment_zero_idx]\n",
    "            misaligned_xxtoen_scores = [xxtoen_sample[lang]['scores'][i] for i in alignment_zero_idx]\n",
    "\n",
    "            entoxx_u_stat, entoxx_u_p_value = stats.mannwhitneyu(aligned_entoxx_scores, misaligned_entoxx_scores, alternative='greater')\n",
    "            entoxx_t_stat, entoxx_t_p_value = stats.ttest_ind(aligned_entoxx_scores, misaligned_entoxx_scores, alternative='greater')\n",
    "\n",
    "            entoxxdelta_results[lang]['delta'] = np.mean(aligned_entoxx_scores)-np.mean(misaligned_entoxx_scores)\n",
    "            entoxxdelta_results[lang]['Utest_pval'] = entoxx_u_p_value\n",
    "            entoxxdelta_results[lang]['ttest_pval'] = entoxx_t_p_value\n",
    "            entoxxdelta_results[lang]['N_aligned'] = len(alignment_one_idx)\n",
    "            entoxxdelta_results[lang]['N_nonaligned'] = len(alignment_zero_idx)\n",
    "\n",
    "            xxtoen_u_stat, xxtoen_u_p_value = stats.mannwhitneyu(aligned_xxtoen_scores, misaligned_xxtoen_scores, alternative='greater')\n",
    "            xxtoen_t_stat, xxtoen_t_p_value = stats.ttest_ind(aligned_xxtoen_scores, misaligned_xxtoen_scores, alternative='greater')\n",
    "\n",
    "            xxtoendelta_results[lang]['delta'] = np.mean(aligned_xxtoen_scores)-np.mean(misaligned_xxtoen_scores)\n",
    "            xxtoendelta_results[lang]['Utest_pval'] = xxtoen_u_p_value\n",
    "            xxtoendelta_results[lang]['ttest_pval'] = xxtoen_t_p_value\n",
    "            xxtoendelta_results[lang]['N_aligned'] = len(alignment_one_idx)\n",
    "            xxtoendelta_results[lang]['N_nonaligned'] = len(alignment_zero_idx)\n",
    "\n",
    "    return entoxxdelta_results, xxtoendelta_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entoxx_sample, xxtoen_sample = load_translation_sample('belebele', 'Llama3.1', 'flores_passage')\n",
    "entoxxdelta_results, xxtoendelta_results = analyze_sample_level_translation(entoxx_sample, xxtoen_sample, 'belebele')\n",
    "\n",
    "# Convert entoxxdelta_results to a DataFrame\n",
    "entoxx_delta_df = pd.DataFrame.from_dict(entoxxdelta_results, orient='index').reset_index()\n",
    "xxtoen_delta_df = pd.DataFrame.from_dict(xxtoendelta_results, orient='index').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "entoxx_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "xxtoen_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "entoxx_delta_df.to_excel(\"../../../../Images_DALI/belebele_plots/entoxx_samplelevel_belebele_delta.xlsx\", index=False)\n",
    "xxtoen_delta_df.to_excel(\"../../../../Images_DALI/belebele_plots/xxtoen_samplelevel_belebele_delta.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entoxx_sample, xxtoen_sample = load_translation_sample('flores', 'Llama3.1', 'sentence')\n",
    "entoxxdelta_results, xxtoendelta_results = analyze_sample_level_translation(entoxx_sample, xxtoen_sample, 'flores')\n",
    "\n",
    "# Convert entoxxdelta_results to a DataFrame\n",
    "entoxx_delta_df = pd.DataFrame.from_dict(entoxxdelta_results, orient='index').reset_index()\n",
    "xxtoen_delta_df = pd.DataFrame.from_dict(xxtoendelta_results, orient='index').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "entoxx_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "xxtoen_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "entoxx_delta_df.to_excel(\"../../../../Images_DALI/belebele_plots/entoxx_samplelevel_flores_delta.xlsx\", index=False)\n",
    "xxtoen_delta_df.to_excel(\"../../../../Images_DALI/belebele_plots/xxtoen_samplelevel_flores_delta.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "entoxx_sample_input1, xxtoen_sample_input1 = load_translation_sample('xstorycloze', 'Llama3.1', 'input_sentence_1')\n",
    "entoxx_sample_input2, xxtoen_sample_input2 = load_translation_sample('xstorycloze', 'Llama3.1', 'input_sentence_2')\n",
    "entoxx_sample_input3, xxtoen_sample_input3 = load_translation_sample('xstorycloze', 'Llama3.1', 'input_sentence_3')\n",
    "entoxx_sample_input4, xxtoen_sample_input4 = load_translation_sample('xstorycloze', 'Llama3.1', 'input_sentence_4')\n",
    "\n",
    "entoxx_sample_premise = defaultdict(dict)\n",
    "xxtoen_sample_premise = defaultdict(dict)\n",
    "\n",
    "for lang in entoxx_sample_input1.keys():\n",
    "    entoxx_sample_premise[lang]['scores'] = []\n",
    "    \n",
    "    entoxx_sample_premise[lang]['scores'].extend(entoxx_sample_input1[lang]['scores'])\n",
    "    entoxx_sample_premise[lang]['scores'].extend(entoxx_sample_input2[lang]['scores'])\n",
    "    entoxx_sample_premise[lang]['scores'].extend(entoxx_sample_input3[lang]['scores'])\n",
    "    entoxx_sample_premise[lang]['scores'].extend(entoxx_sample_input4[lang]['scores'])\n",
    "    \n",
    "    xxtoen_sample_premise[lang]['scores'] = []\n",
    "    \n",
    "    xxtoen_sample_premise[lang]['scores'].extend(xxtoen_sample_input1[lang]['scores'])\n",
    "    xxtoen_sample_premise[lang]['scores'].extend(xxtoen_sample_input2[lang]['scores'])\n",
    "    xxtoen_sample_premise[lang]['scores'].extend(xxtoen_sample_input3[lang]['scores'])\n",
    "    xxtoen_sample_premise[lang]['scores'].extend(xxtoen_sample_input4[lang]['scores'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entoxxdelta_results, xxtoendelta_results = analyze_sample_level_translation(entoxx_sample_premise, xxtoen_sample_premise, 'xstorycloze')\n",
    "\n",
    "# Convert entoxxdelta_results to a DataFrame\n",
    "entoxx_delta_df = pd.DataFrame.from_dict(entoxxdelta_results, orient='index').reset_index()\n",
    "xxtoen_delta_df = pd.DataFrame.from_dict(xxtoendelta_results, orient='index').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "entoxx_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "xxtoen_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "entoxx_delta_df.to_excel(\"../../../../Images_DALI/xstorycloze_plots/entoxx_samplelevel_premise_delta.xlsx\", index=False)\n",
    "xxtoen_delta_df.to_excel(\"../../../../Images_DALI/xstorycloze_plots/xxtoen_samplelevel_premise_delta.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entoxx_sample, xxtoen_sample = load_translation_sample('xcopa', 'Llama3.1', 'premise')\n",
    "entoxxdelta_results, xxtoendelta_results = analyze_sample_level_translation(entoxx_sample, xxtoen_sample, 'xcopa')\n",
    "\n",
    "# Convert entoxxdelta_results to a DataFrame\n",
    "entoxx_delta_df = pd.DataFrame.from_dict(entoxxdelta_results, orient='index').reset_index()\n",
    "xxtoen_delta_df = pd.DataFrame.from_dict(xxtoendelta_results, orient='index').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "entoxx_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "xxtoen_delta_df.rename(columns={'index': 'Language', \n",
    "                                 'delta': 'Delta', \n",
    "                                 'Utest_pval': 'Utest_pval', \n",
    "                                 'ttest_pval': 'Ttest_pval', \n",
    "                                 'N_aligned': 'N_aligned', \n",
    "                                 'N_nonaligned': 'N_nonaligned'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "entoxx_delta_df.to_excel(\"../../../../Images_DALI/xcopa_plots/entoxx_samplelevel_premise_delta.xlsx\", index=False)\n",
    "xxtoen_delta_df.to_excel(\"../../../../Images_DALI/xcopa_plots/xxtoen_samplelevel_premise_delta.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
