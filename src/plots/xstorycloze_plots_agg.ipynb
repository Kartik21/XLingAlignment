{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset #load_dataset from Huggingface\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata, spearmanr, pearsonr\n",
    "import statsmodels.stats.proportion as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_DICT = {'arabic': 'ar', \n",
    "                 'english': 'en', \n",
    "                 'spanish': 'es',\n",
    "                 'basque': 'eu',\n",
    "                 'indonesian': 'id',\n",
    "                 'burmese': 'my',\n",
    "                 'russian': 'ru',\n",
    "                 'telugu': 'te',\n",
    "                 'chinese': 'zh',\n",
    "                 'swahili': 'sw',\n",
    "                 'hindi': 'hi'}\n",
    "\n",
    "LANGUAGE = ['arabic', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'chinese', 'swahili']\n",
    "INPUT_FIELD = ['input_sentence_1', 'input_sentence_2', 'input_sentence_3', 'input_sentence_4', 'sentence_quiz1', 'sentence_quiz2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_outputs(lang, dataset='xstorycloze', model='Llama3.1', mode='normalized', shot=0):\n",
    "    model_dict = {'Llama3.1': 'meta-llama__Llama-3.1-8B'}\n",
    "    lang_code = LANG_DICT[lang]\n",
    "    model_code = model_dict[model]\n",
    "    if dataset == 'xstorycloze':\n",
    "        hf = load_dataset(\"juletxara/xstory_cloze\", lang_code)\n",
    "        answer_right_ending = hf['eval']['answer_right_ending']\n",
    "        answer = []\n",
    "        for i in range(len(answer_right_ending)):\n",
    "            answer.append(answer_right_ending[i])\n",
    "    \n",
    "    if mode == 'normalized':\n",
    "        accuracy_data_path = f'../../accuracy_outputs/{model}/{dataset}_normalized_{shot}shot/{lang_code}/{model_code}/'\n",
    "    else:\n",
    "        accuracy_data_path = f'../../accuracy_outputs/{model}/{dataset}_unnormalized_{shot}shot/{lang_code}/{model_code}/'\n",
    "    # Find the .jsonl file in the directory\n",
    "    jsonl_file = [f for f in os.listdir(accuracy_data_path) if f.endswith('.jsonl')][0]\n",
    "    file_path = os.path.join(accuracy_data_path, jsonl_file)\n",
    "    # Read the jsonl file line by line\n",
    "    accuracy_results = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            accuracy_results.append(json.loads(line))\n",
    "\n",
    "    resps = [item['resps'] for item in accuracy_results]\n",
    "    accuracy = [item['acc'] for item in accuracy_results]\n",
    "    score_diff = []\n",
    "    for i in range(len(answer)):\n",
    "        if answer[i]==1:\n",
    "            score_diff.append(float(resps[i][0][0][0]) - float(resps[i][1][0][0]))\n",
    "        if answer[i]==2:\n",
    "            score_diff.append(float(resps[i][1][0][0]) - float(resps[i][0][0][0]))\n",
    "    return score_diff, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(correct_eng, incorrect_eng, correct_lang, incorrect_lang):\n",
    "    TP = len(set(correct_eng) & set(correct_lang))\n",
    "    FN = len(set(incorrect_eng) & set(correct_lang))\n",
    "    FP = len(set(correct_eng) & set(incorrect_lang))\n",
    "    TN = len(set(incorrect_eng) & set(incorrect_lang))\n",
    "    \n",
    "    return np.array([[TP, FN], [FP, TN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = defaultdict(dict)\n",
    "score_diff_dict = defaultdict(dict)\n",
    "plt.rcParams[\"savefig.format\"] = 'pdf'\n",
    "plt.rcParams['font.family'] = 'Palatino'\n",
    "\n",
    "conf_matrices = defaultdict(dict)\n",
    "\n",
    "score_diff_dict['english'], acc_dict['english'] = get_accuracy_outputs('english', 'xstorycloze', 'Llama3.1', mode='unnormalized', shot=5)\n",
    "\n",
    "for lang in ['spanish']:\n",
    "    score_diff_dict[lang],acc_dict[lang]  = get_accuracy_outputs(lang, 'xstorycloze', 'Llama3.1', mode='unnormalized', shot=5)\n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "       \n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "\n",
    "    # Example language data (Replace with actual lists)\n",
    "    conf_matrices[lang] = compute_confusion_matrix(correct_id_eng, incorrect_id_eng, correct_id_lang, incorrect_id_lang)\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "\n",
    "''' \n",
    "# Plot multiple confusion matrices\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))  # 2 rows, 5 columns\n",
    "\n",
    "for ax, (lang, matrix) in zip(axes.flat, conf_matrices.items()):\n",
    "\n",
    "#for lang, matrix in conf_matrices.items():    \n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Correct_Eng\", \"Incorrect_Eng\"], \n",
    "                yticklabels=[\"Correct_Lang\", \"Incorrect_Lang\"], annot_kws={\"size\": 14}, ax=ax)\n",
    "    #plt.set_title(lang)\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "for lang, matrix in conf_matrices.items():    \n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Correct_Eng\", \"Incorrect_Eng\"], \n",
    "                yticklabels=[\"Correct_Lang\", \"Incorrect_Lang\"], annot_kws={\"size\": 16})\n",
    "    plt.xticks(size=16)\n",
    "    plt.yticks(size=16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(fname='../../../../Images_DALI/cohort_level_analysis_illustrated.pdf')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DALI(dataset, lang, model, mode):\n",
    "    if mode == 'DALI':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali/BAS_{lang}_lasttoken.json'\n",
    "    if mode == 'DALIStrong':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali_strong/DALI_{lang}_lasttoken.json'\n",
    "\n",
    "    if mode == 'MEXAFlores':\n",
    "        flores_dict = {'arabic': 'arb_Arab', \n",
    "                       'spanish': 'spa_Latn',\n",
    "                        'basque': 'eus_Latn',\n",
    "                         'hindi': 'hin_Deva',\n",
    "                          'indonesian': 'ind_Latn',\n",
    "                           'burmese': 'mya_Mymr',\n",
    "                            'russian': 'rus_Cyrl',\n",
    "                             'telugu': 'tel_Telu',\n",
    "                              'chinese': 'zho_Hans',\n",
    "                               'swahili': 'swh_Latn'}\n",
    "        \n",
    "        lang_code = flores_dict[lang]\n",
    "\n",
    "        DAS_path = f'../../alignment_outputs/{model}/flores_mexa/{lang_code}.json'\n",
    "    if mode == 'MEXATask':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang}.json'\n",
    "    with open(DAS_path) as f:\n",
    "        lang_DAS = json.load(f)\n",
    "    return lang_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_agg(dataset, model, field):\n",
    "    if dataset == 'flores':\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}_100/sentence/entoxx_{model}_{dataset}_COMET.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}_100/sentence/xxtoen_{model}_{dataset}_COMET.json'\n",
    "        list_of_languages = ['modern standard arabic', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'simplified chinese', 'swahili']\n",
    "        lang_key = {'modern standard arabic': 'arabic', 'spanish': 'spanish', 'basque': 'basque', 'hindi': 'hindi', 'indonesian': 'indonesian', 'burmese': 'burmese', 'russian': 'russian', 'telugu': 'telugu', 'simplified chinese': 'chinese', 'swahili': 'swahili'}\n",
    "\n",
    "        with open(entoxxpath) as f:\n",
    "            entoxx_COMET = json.load(f)\n",
    "    \n",
    "        with open(xxtoenpath) as f:\n",
    "            xxtoen_COMET = json.load(f)\n",
    "        \n",
    "        entoxx_COMET_filtered = {}\n",
    "        xxtoen_COMET_filtered = {}\n",
    "        for k,v in entoxx_COMET.items():\n",
    "            \n",
    "            if k in list_of_languages:\n",
    "                entoxx_COMET_filtered[lang_key[k]] = v[0]\n",
    "        for k,v in xxtoen_COMET.items():\n",
    "            if k in list_of_languages:\n",
    "                xxtoen_COMET_filtered[lang_key[k]] = v[0]\n",
    "\n",
    "    if dataset == 'xstorycloze':\n",
    "        entoxxpath = f'../../translation_outputs/{model}/{dataset}/{field}/entoxx_{model}_{dataset}_COMET.json'\n",
    "        xxtoenpath = f'../../translation_outputs/{model}/{dataset}/{field}/xxtoen_{model}_{dataset}_COMET.json'\n",
    "\n",
    "        with open(entoxxpath) as f:\n",
    "            entoxx_COMET = json.load(f)\n",
    "        with open(xxtoenpath) as f:\n",
    "            xxtoen_COMET = json.load(f)\n",
    "        entoxx_COMET_filtered = {}\n",
    "        xxtoen_COMET_filtered = {}\n",
    "\n",
    "        for k,v in entoxx_COMET.items():\n",
    "            entoxx_COMET_filtered[k] = v[0]\n",
    "        for k,v in xxtoen_COMET.items():\n",
    "            xxtoen_COMET_filtered[k] = v[0]           \n",
    "    return entoxx_COMET_filtered, xxtoen_COMET_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies and get DAS scores for each language\n",
    "def plot_alignment_by_layers(mode):\n",
    "    accuracies = {}\n",
    "    max_das_scores = {}\n",
    "    mean_das_scores = {}\n",
    "    all_das_avgs = {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.rcParams['font.family'] = 'Palatino'\n",
    "\n",
    "\n",
    "    for lang in LANGUAGE:\n",
    "        # Calculate accuracy\n",
    "        accuracies[lang] = (sum(acc_dict[lang])/len(acc_dict[lang])) \n",
    "        # Get DAS data for this language\n",
    "        lang_DAS = plot_DALI('xstorycloze', lang, 'Llama3.1', mode)\n",
    "        \n",
    "        if mode == 'DALI' or mode == 'DALIStrong':\n",
    "            lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                        for outer_k, inner_v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_scores = [lang_DAS[sample][layer] for sample in lang_DAS.keys()]\n",
    "                layer_avgs.append(np.mean(layer_scores))\n",
    "\n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "\n",
    "            \n",
    "        else: \n",
    "            lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_avgs.append(np.mean(lang_DAS[layer]))      \n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            \n",
    "\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.tick_params(axis='both',labelsize=14)\n",
    "    ax.set_ylabel(f'{mode}')\n",
    "    #ax.set_title('DALI Scores Across Layers (story completion)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_ylim(0,1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return accuracies, max_das_scores, mean_das_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entoxx_flores_agg, xxtoen_flores_agg = load_translation_agg('flores', 'Llama3.1', field=None)\n",
    "entoxx_input1, xxtoen_input1 = load_translation_agg('xstorycloze', 'Llama3.1', field='input_sentence_1')\n",
    "entoxx_input2, xxtoen_input2 = load_translation_agg('xstorycloze', 'Llama3.1', field='input_sentence_2')\n",
    "entoxx_input3, xxtoen_input3 = load_translation_agg('xstorycloze', 'Llama3.1', field='input_sentence_3')\n",
    "entoxx_input4, xxtoen_input4 = load_translation_agg('xstorycloze', 'Llama3.1', field='input_sentence_4')\n",
    "entoxx_end1, xxtoen_end1 = load_translation_agg('xstorycloze', 'Llama3.1', field='sentence_quiz1')\n",
    "entoxx_end2, xxtoen_end2 = load_translation_agg('xstorycloze', 'Llama3.1', field='sentence_quiz2')\n",
    "\n",
    "entoxx_premise_agg = defaultdict()\n",
    "xxtoen_premise_agg = defaultdict()\n",
    "\n",
    "\n",
    "entoxx_dataset_agg = {}\n",
    "xxtoen_dataset_agg = {}\n",
    "\n",
    "for k in entoxx_input1.keys():\n",
    "    entoxx_dataset_agg[k] = np.mean([entoxx_input1[k], entoxx_input2[k], entoxx_input3[k], entoxx_input4[k], entoxx_end1[k], entoxx_end2[k]])\n",
    "    xxtoen_dataset_agg[k] = np.mean([xxtoen_input1[k], xxtoen_input2[k], xxtoen_input3[k], xxtoen_input4[k], xxtoen_end1[k], xxtoen_end2[k]])\n",
    "\n",
    "for k in entoxx_input1.keys():\n",
    "    entoxx_premise_agg[k] = np.mean([entoxx_input1[k], entoxx_input2[k], entoxx_input3[k], entoxx_input4[k]])\n",
    "    xxtoen_premise_agg[k] = np.mean([xxtoen_input1[k], xxtoen_input2[k], xxtoen_input3[k], xxtoen_input4[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_sample(dataset, model, field):\n",
    "    entoxxpath = f'../../translation_outputs/{model}/{dataset}/{field}/entoxx_{model}_{dataset}_COMET_sample.json'\n",
    "    xxtoenpath = f'../../translation_outputs/{model}/{dataset}/{field}/xxtoen_{model}_{dataset}_COMET_sample.json'\n",
    "\n",
    "    with open(entoxxpath) as f:\n",
    "        entoxx_COMET_sample = json.load(f)\n",
    "    with open(xxtoenpath) as f:\n",
    "        xxtoen_COMET_sample = json.load(f)\n",
    "\n",
    "    return entoxx_COMET_sample, xxtoen_COMET_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranked_correlation(entoxx_dataset_agg, \n",
    "                            xxtoen_dataset_agg, \n",
    "                            entoxx_flores_agg, \n",
    "                            xxtoen_flores_agg,\n",
    "                            entoxx_premise_agg,\n",
    "                            xxtoen_premise_agg,\n",
    "                            accuracies,\n",
    "                            dali,\n",
    "                            dali_strong,\n",
    "                            mexa_flores,\n",
    "                            mexa_dataset,\n",
    "                            mode = \"translation-accuracy\"):\n",
    "    \n",
    "    accuracies_rank = rankdata([accuracies[lang] for lang in LANGUAGE])\n",
    "    entoxx_flores_rank = rankdata([entoxx_flores_agg[lang] for lang in LANGUAGE])\n",
    "    xxtoen_flores_rank = rankdata([xxtoen_flores_agg[lang] for lang in LANGUAGE])\n",
    "    entoxx_dataset_rank = rankdata([entoxx_dataset_agg[lang] for lang in LANGUAGE])\n",
    "    xxtoen_dataset_rank = rankdata([xxtoen_dataset_agg[lang] for lang in LANGUAGE])\n",
    "\n",
    "    entoxx_premise_rank = rankdata([entoxx_premise_agg[lang] for lang in LANGUAGE])\n",
    "    xxtoen_premise_rank = rankdata([xxtoen_premise_agg[lang] for lang in LANGUAGE])\n",
    "\n",
    "\n",
    "\n",
    "    dali_rank = rankdata([dali[lang] for lang in LANGUAGE])\n",
    "    dali_strong_rank = rankdata([dali_strong[lang] for lang in LANGUAGE])\n",
    "    mexa_flores_rank = rankdata([mexa_flores[lang] for lang in LANGUAGE])\n",
    "    mexa_dataset_rank = rankdata([mexa_dataset[lang] for lang in LANGUAGE])\n",
    "\n",
    "    if mode == \"translation-accuracy\":\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        corr_entoxx_flores_acc, _ = spearmanr(entoxx_flores_rank, accuracies_rank)\n",
    "        corr_xxtoen_flores_acc, _ = spearmanr(xxtoen_flores_rank, accuracies_rank)\n",
    "        corr_entoxx_dataset_acc, _ = spearmanr(entoxx_dataset_rank, accuracies_rank)\n",
    "        corr_xxtoen_dataset_acc, _ = spearmanr(xxtoen_dataset_rank, accuracies_rank)\n",
    "\n",
    "        slope_entoxx_flores_acc, b_entoxx_flores_acc = np.polyfit(entoxx_dataset_rank, accuracies_rank, 1)\n",
    "        slope_xxtoen_flores_acc, b_xxtoen_flores_acc = np.polyfit(xxtoen_dataset_rank, accuracies_rank, 1)\n",
    "        slope_entoxx_dataset_acc, b_entoxx_dataset_acc = np.polyfit(entoxx_dataset_rank, accuracies_rank, 1)\n",
    "        slope_xxtoen_dataset_acc, b_xxtoen_dataset_acc = np.polyfit(xxtoen_dataset_rank, accuracies_rank, 1)\n",
    "\n",
    "        axes[0][0].scatter(entoxx_flores_rank, accuracies_rank, alpha=0.7, color='blue')\n",
    "        axes[0][0].plot(entoxx_flores_rank, np.array(entoxx_flores_rank) * slope_entoxx_flores_acc + b_entoxx_flores_acc, color='red', label=f'Linear fit: y={slope_entoxx_flores_acc:.2f}x+{b_entoxx_flores_acc:.2f}')\n",
    "        axes[0][0].text(min(entoxx_flores_rank)+1, max(accuracies_rank)-1., f'Spearman r: {corr_entoxx_flores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][0].set_xlabel('COMET Flores (En->XX)')\n",
    "        axes[0][0].set_ylabel('Accuracy')\n",
    "        axes[0][0].set_title(f'En->XX Flores COMET vs Accuracy')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][0].grid(True)\n",
    "\n",
    "        axes[0][1].scatter(xxtoen_flores_rank, accuracies_rank, alpha=0.7, color='teal')\n",
    "        axes[0][1].plot(xxtoen_flores_rank, np.array(xxtoen_flores_rank) * slope_xxtoen_flores_acc + b_xxtoen_flores_acc, color='red', label=f'Linear fit: y={slope_xxtoen_flores_acc:.2f}x+{b_xxtoen_flores_acc:.2f}')\n",
    "        axes[0][1].text(min(xxtoen_flores_rank)+1, max(accuracies_rank)-1, f'Spearman r: {corr_xxtoen_flores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][1].set_xlabel('COMET Flores (XX->En)')\n",
    "        axes[0][1].set_ylabel('Accuracy')\n",
    "        axes[0][1].set_title(f'XX->En Flores COMET vs Accuracy')\n",
    "        #axes[0][1].set_xlim(0,1)\n",
    "        #axes[0][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][1].grid(True)\n",
    "\n",
    "        axes[1][0].scatter(entoxx_dataset_rank, accuracies_rank, alpha=0.7, color='brown')\n",
    "        axes[1][0].plot(entoxx_dataset_rank, np.array(entoxx_dataset_rank) * slope_entoxx_dataset_acc + b_entoxx_dataset_acc, color='red', label=f'Linear fit: y={slope_entoxx_dataset_acc:.2f}x+{b_entoxx_dataset_acc:.2f}')\n",
    "        axes[1][0].text(min(entoxx_dataset_rank)+1, max(accuracies_rank)-1, f'Spearman r: {corr_entoxx_dataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][0].set_xlabel('COMET Dataset (En->XX)')\n",
    "        axes[1][0].set_ylabel('Accuracy')\n",
    "        axes[1][0].set_title(f'En->XX Dataset COMET vs Accuracy')\n",
    "        #axes[1][0].set_xlim(0,1)\n",
    "        #axes[1][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][0].grid(True)\n",
    "\n",
    "        axes[1][1].scatter(xxtoen_dataset_rank, accuracies_rank, alpha=0.7, color='green')\n",
    "        axes[1][1].plot(xxtoen_dataset_rank, np.array(xxtoen_dataset_rank) * slope_xxtoen_dataset_acc + b_xxtoen_dataset_acc, color='red', label=f'Linear fit: y={slope_xxtoen_dataset_acc:.2f}x+{b_xxtoen_dataset_acc:.2f}')\n",
    "        axes[1][1].text(min(xxtoen_dataset_rank)+1, max(accuracies_rank)-1, f'Spearman r: {corr_xxtoen_dataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][1].set_xlabel('COMET Dataset (XX->En)')\n",
    "        axes[1][1].set_ylabel('Accuracy')\n",
    "        axes[1][1].set_title(f'XX->En Dataset COMET vs Accuracy')\n",
    "        #axes[1][1].set_xlim(0,1)\n",
    "        #axes[1][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][1].grid(True)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Rank Correlation: Translation Quality vs Accuracy in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/rankcorrel_translationquality_vs_accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    if mode == \"alignment-translation\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        \n",
    "        corr_entoxx_task_mexa, _ = spearmanr(mexa_dataset_rank, entoxx_premise_rank)\n",
    "        corr_xxtoen_task_mexa, _ = spearmanr(mexa_dataset_rank, xxtoen_premise_rank)\n",
    "\n",
    "        slope_entoxx_task_mexa, b_entoxx_task_mexa = np.polyfit(mexa_dataset_rank, entoxx_premise_rank, 1)\n",
    "        slope_xxtoen_task_mexa, b_xxtoen_task_mexa = np.polyfit(mexa_dataset_rank, xxtoen_premise_rank, 1)\n",
    "\n",
    "        axes[0].scatter(mexa_dataset_rank, entoxx_premise_rank, alpha=0.7, color='blue')\n",
    "        axes[0].plot(entoxx_premise_rank, np.array(entoxx_premise_rank) * slope_entoxx_task_mexa + b_entoxx_task_mexa, color='red', label=f'Linear fit: y={slope_entoxx_task_mexa:.2f}x+{b_entoxx_task_mexa:.2f}')\n",
    "        axes[0].text(min(mexa_dataset_rank)+1, max(entoxx_premise_rank)-1., f'Spearman r: {corr_entoxx_task_mexa:.2f}', fontsize=12, color='red')\n",
    "        axes[0].set_xlabel('MEXA Task')\n",
    "        axes[0].set_ylabel('En->XX Premise COMET')\n",
    "        axes[0].set_title(f'MEXA Task vs En->XX Premise')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        axes[1].scatter(mexa_dataset_rank, xxtoen_premise_rank, alpha=0.7, color='blue')\n",
    "        axes[1].plot(xxtoen_premise_rank, np.array(xxtoen_premise_rank) * slope_xxtoen_task_mexa + b_xxtoen_task_mexa, color='red', label=f'Linear fit: y={slope_xxtoen_task_mexa:.2f}x+{b_xxtoen_task_mexa:.2f}')\n",
    "        axes[1].text(min(mexa_dataset_rank)+1, max(xxtoen_premise_rank)-1., f'Spearman r: {corr_xxtoen_task_mexa:.2f}', fontsize=12, color='red')\n",
    "        axes[1].set_xlabel('MEXA Task')\n",
    "        axes[1].set_ylabel('XX->En Premise COMET')\n",
    "        axes[1].set_title(f'MEXA Task vs XX->En Premise')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Rank Correlation: MEXA (Task) vs Translation Quality in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/rankcorrel_mexa_vs_translationquality')\n",
    "        plt.show()\n",
    "\n",
    "    if mode == \"alignment-accuracy\":\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        corr_dali_acc, _ = spearmanr(dali_rank, accuracies_rank)\n",
    "        corr_dalistrong_acc, _ = spearmanr(dali_strong_rank, accuracies_rank)\n",
    "        corr_mexaflores_acc, _ = spearmanr(mexa_flores_rank, accuracies_rank)\n",
    "        corr_mexadataset_acc, _ = spearmanr(mexa_dataset_rank, accuracies_rank)\n",
    "\n",
    "        slope_dali_acc, b_dali_acc = np.polyfit(dali_rank, accuracies_rank, 1)\n",
    "        slope_dalistrong_acc, b_dalistrong_acc = np.polyfit(dali_strong_rank, accuracies_rank, 1)\n",
    "        slope_mexaflores_acc, b_mexaflores_acc = np.polyfit(mexa_flores_rank, accuracies_rank, 1)\n",
    "        slope_mexadataset_acc, b_mexadataset_acc = np.polyfit(mexa_dataset_rank, accuracies_rank, 1)\n",
    "\n",
    "        axes[0][0].scatter(dali_rank, accuracies_rank, alpha=0.7, color='blue')\n",
    "        axes[0][0].plot(dali_rank, np.array(dali_rank) * slope_dali_acc + b_dali_acc, color='red', label=f'Linear fit: y={slope_dali_acc:.2f}x+{b_dali_acc:.2f}')\n",
    "        axes[0][0].text(min(dali_rank)+1, max(accuracies_rank)-1., f'Spearman r: {corr_dali_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][0].set_xlabel('DALI (Maxpool)')\n",
    "        axes[0][0].set_ylabel('Accuracy')\n",
    "        axes[0][0].set_title(f'DALI vs Accuracy')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][0].grid(True)\n",
    "\n",
    "        axes[0][1].scatter(dali_strong_rank, accuracies_rank, alpha=0.7, color='blue')\n",
    "        axes[0][1].plot(dali_strong_rank, np.array(dali_strong_rank) * slope_dalistrong_acc + b_dalistrong_acc, color='red', label=f'Linear fit: y={slope_dalistrong_acc:.2f}x+{b_dalistrong_acc:.2f}')\n",
    "        axes[0][1].text(min(dali_strong_rank)+1, max(accuracies_rank)-1., f'Spearman r: {corr_dalistrong_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][1].set_xlabel('DALIStrong (Maxpool)')\n",
    "        axes[0][1].set_ylabel('Accuracy')\n",
    "        axes[0][1].set_title(f'DALIStrong vs Accuracy')\n",
    "        #axes[0][1].set_xlim(0,1)\n",
    "        #axes[0][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][1].grid(True)\n",
    "\n",
    "        axes[1][0].scatter(mexa_flores_rank, accuracies_rank, alpha=0.7, color='blue')\n",
    "        axes[1][0].plot(mexa_flores_rank, np.array(mexa_flores_rank) * slope_mexaflores_acc + b_mexaflores_acc, color='red', label=f'Linear fit: y={slope_mexaflores_acc:.2f}x+{b_mexaflores_acc:.2f}')\n",
    "        axes[1][0].text(min(mexa_flores_rank)+1, max(accuracies_rank)-1., f'Spearman r: {corr_mexaflores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][0].set_xlabel('MEXA Flores (Maxpool)')\n",
    "        axes[1][0].set_ylabel('Accuracy')\n",
    "        axes[1][0].set_title(f'MEXA Flores vs Accuracy')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][0].grid(True)\n",
    "\n",
    "        axes[1][1].scatter(mexa_dataset_rank, accuracies_rank, alpha=0.7, color='blue')\n",
    "        axes[1][1].plot(mexa_dataset_rank, np.array(mexa_dataset_rank) * slope_mexadataset_acc + b_mexadataset_acc, color='red', label=f'Linear fit: y={slope_mexadataset_acc:.2f}x+{b_mexadataset_acc:.2f}')\n",
    "        axes[1][1].text(min(mexa_dataset_rank)+1, max(accuracies_rank)-1., f'Spearman r: {corr_mexadataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][1].set_xlabel('MEXA Dataset (Maxpool)')\n",
    "        axes[1][1].set_ylabel('Accuracy')\n",
    "        axes[1][1].set_title(f'MEXA Dataset vs Accuracy')\n",
    "        #axes[0][0].set_xlim(0,1)\n",
    "        #axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][1].grid(True)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Rank Correlation: Cross-Lingual Alignment vs Accuracy in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/rankcorrel_alignment_vs_accuracy.pdf')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, max_pooled_dali_scores, mean_pooled_dali_scores = plot_alignment_by_layers('DALI')\n",
    "accuracies, max_pooled_dalistrong_scores, mean_pooled_dalistrong_scores = plot_alignment_by_layers('DALIStrong')\n",
    "accuracies, max_pooled_mexaflores_scores, mean_pooled_mexaflores_scores = plot_alignment_by_layers('MEXAFlores')\n",
    "accuracies, max_pooled_mexadataset_scores,mean_pooled_mexadataset_scores = plot_alignment_by_layers('MEXATask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranked_correlation(entoxx_dataset_agg, \n",
    "                        xxtoen_dataset_agg, \n",
    "                        entoxx_flores_agg, \n",
    "                        xxtoen_flores_agg, \n",
    "                        entoxx_premise_agg,\n",
    "                        xxtoen_premise_agg,\n",
    "                        accuracies, \n",
    "                        mean_pooled_dali_scores, \n",
    "                        mean_pooled_dalistrong_scores,\n",
    "                        mean_pooled_mexaflores_scores,\n",
    "                        mean_pooled_mexadataset_scores,\n",
    "                        mode='alignment-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pearson_correlation(entoxx_dataset_agg, \n",
    "                            xxtoen_dataset_agg, \n",
    "                            entoxx_flores_agg, \n",
    "                            xxtoen_flores_agg,\n",
    "                            entoxx_premise_agg,\n",
    "                            xxtoen_premise_agg,\n",
    "                            accuracies,\n",
    "                            dali,\n",
    "                            dali_strong,\n",
    "                            mexa_flores,\n",
    "                            mexa_dataset,\n",
    "                            mode = \"translation-accuracy\"):\n",
    "    \n",
    "    accuracies = [accuracies[lang] for lang in LANGUAGE]\n",
    "    entoxx_flores = [entoxx_flores_agg[lang] for lang in LANGUAGE]\n",
    "    xxtoen_flores = [xxtoen_flores_agg[lang] for lang in LANGUAGE]\n",
    "    entoxx_dataset = [entoxx_dataset_agg[lang] for lang in LANGUAGE]\n",
    "    xxtoen_dataset = [xxtoen_dataset_agg[lang] for lang in LANGUAGE]\n",
    "\n",
    "    entoxx_premise = [entoxx_premise_agg[lang] for lang in LANGUAGE]\n",
    "    xxtoen_premise = [xxtoen_premise_agg[lang] for lang in LANGUAGE]\n",
    "\n",
    "\n",
    "\n",
    "    dali = [dali[lang] for lang in LANGUAGE]\n",
    "    dali_strong = [dali_strong[lang] for lang in LANGUAGE]\n",
    "    mexa_flores = [mexa_flores[lang] for lang in LANGUAGE]\n",
    "    mexa_dataset = [mexa_dataset[lang] for lang in LANGUAGE]\n",
    "    \n",
    "\n",
    "    if mode == \"translation-accuracy\":\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 6))    \n",
    "        # Calculate Pearson correlation coefficient\n",
    "        corr_entoxx_flores_acc, _ = pearsonr(entoxx_flores, accuracies)\n",
    "        corr_xxtoen_flores_acc, _ = pearsonr(xxtoen_flores, accuracies)\n",
    "        corr_entoxx_dataset_acc, _ = pearsonr(entoxx_dataset, accuracies)\n",
    "        corr_xxtoen_dataset_acc, _ = pearsonr(xxtoen_dataset, accuracies)\n",
    "\n",
    "        slope_entoxx_flores_acc, b_entoxx_flores_acc = np.polyfit(entoxx_dataset, accuracies, 1)\n",
    "        slope_xxtoen_flores_acc, b_xxtoen_flores_acc = np.polyfit(xxtoen_dataset, accuracies, 1)\n",
    "        slope_entoxx_dataset_acc, b_entoxx_dataset_acc = np.polyfit(entoxx_dataset, accuracies, 1)\n",
    "        slope_xxtoen_dataset_acc, b_xxtoen_dataset_acc = np.polyfit(xxtoen_dataset, accuracies, 1)\n",
    "        \n",
    "\n",
    "        axes[0][0].scatter(entoxx_flores, accuracies, alpha=0.7, color='blue')\n",
    "        axes[0][0].plot(entoxx_flores, np.array(entoxx_flores) * slope_entoxx_flores_acc + b_entoxx_flores_acc, color='red', label=f'Linear fit: y={slope_entoxx_flores_acc:.2f}x+{b_entoxx_flores_acc:.2f}')\n",
    "        axes[0][0].text(0.3,0.7, f'Pearson r: {corr_entoxx_flores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][0].set_xlabel('COMET Flores (En->XX)')\n",
    "        axes[0][0].set_ylabel('Accuracy')\n",
    "        axes[0][0].set_title(f'En->XX Flores COMET vs Accuracy')\n",
    "        axes[0][0].set_xlim(0,1)\n",
    "        axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][0].grid(True)\n",
    "\n",
    "        axes[0][1].scatter(xxtoen_flores, accuracies, alpha=0.7, color='teal')\n",
    "        axes[0][1].plot(xxtoen_flores, np.array(xxtoen_flores) * slope_xxtoen_flores_acc + b_xxtoen_flores_acc, color='red', label=f'Linear fit: y={slope_xxtoen_flores_acc:.2f}x+{b_xxtoen_flores_acc:.2f}')\n",
    "        axes[0][1].text(0.3,0.7, f'Pearson r: {corr_xxtoen_flores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][1].set_xlabel('COMET Flores (XX->En)')\n",
    "        axes[0][1].set_ylabel('Accuracy')\n",
    "        axes[0][1].set_title(f'XX->En Flores COMET vs Accuracy')\n",
    "        axes[0][1].set_xlim(0,1)\n",
    "        axes[0][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][1].grid(True)\n",
    "\n",
    "        axes[1][0].scatter(entoxx_dataset, accuracies, alpha=0.7, color='brown')\n",
    "        axes[1][0].plot(entoxx_dataset, np.array(entoxx_dataset) * slope_entoxx_dataset_acc + b_entoxx_dataset_acc, color='red', label=f'Linear fit: y={slope_entoxx_dataset_acc:.2f}x+{b_entoxx_dataset_acc:.2f}')\n",
    "        axes[1][0].text(0.3,0.7, f'Pearson r: {corr_entoxx_dataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][0].set_xlabel('COMET Dataset (En->XX)')\n",
    "        axes[1][0].set_ylabel('Accuracy')\n",
    "        axes[1][0].set_title(f'En->XX Dataset COMET vs Accuracy')\n",
    "        axes[1][0].set_xlim(0,1)\n",
    "        axes[1][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][0].grid(True)\n",
    "\n",
    "        axes[1][1].scatter(xxtoen_dataset, accuracies, alpha=0.7, color='green')\n",
    "        axes[1][1].plot(xxtoen_dataset, np.array(xxtoen_dataset) * slope_xxtoen_dataset_acc + b_xxtoen_dataset_acc, color='red', label=f'Linear fit: y={slope_xxtoen_dataset_acc:.2f}x+{b_xxtoen_dataset_acc:.2f}')\n",
    "        axes[1][1].text(0.3,0.7, f'Pearson r: {corr_xxtoen_dataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][1].set_xlabel('COMET Dataset (XX->En)')\n",
    "        axes[1][1].set_ylabel('Accuracy')\n",
    "        axes[1][1].set_title(f'XX->En Dataset COMET vs Accuracy')\n",
    "        axes[1][1].set_xlim(0,1)\n",
    "        axes[1][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][1].grid(True)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Pearson Correlation: Translation Quality vs Accuracy in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/correl_translationquality_vs_accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    if mode == \"alignment-translation\":\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        \n",
    "        corr_entoxx_task_mexa, _ = pearsonr(mexa_dataset, entoxx_premise)\n",
    "        corr_xxtoen_task_mexa, _ = pearsonr(mexa_dataset, xxtoen_premise)\n",
    "\n",
    "        slope_entoxx_task_mexa, b_entoxx_task_mexa = np.polyfit(mexa_dataset, entoxx_premise, 1)\n",
    "        slope_xxtoen_task_mexa, b_xxtoen_task_mexa = np.polyfit(mexa_dataset, xxtoen_premise, 1)\n",
    "\n",
    "        axes[0].scatter(mexa_dataset, entoxx_premise, alpha=0.7, color='blue')\n",
    "        axes[0].plot(entoxx_premise, np.array(entoxx_premise) * slope_entoxx_task_mexa + b_entoxx_task_mexa, color='red', label=f'Linear fit: y={slope_entoxx_task_mexa:.2f}x+{b_entoxx_task_mexa:.2f}')\n",
    "        axes[0].text(0.3, 0.7, f'Pearson r: {corr_entoxx_task_mexa:.2f}', fontsize=12, color='red')\n",
    "        axes[0].set_xlabel('MEXA Task')\n",
    "        axes[0].set_ylabel('En->XX Premise COMET')\n",
    "        axes[0].set_title(f'MEXA Task vs En->XX Premise')\n",
    "        axes[0].set_xlim(0,1)\n",
    "        axes[0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        axes[1].scatter(mexa_dataset, xxtoen_premise, alpha=0.7, color='blue')\n",
    "        axes[1].plot(xxtoen_premise, np.array(xxtoen_premise) * slope_xxtoen_task_mexa + b_xxtoen_task_mexa, color='red', label=f'Linear fit: y={slope_xxtoen_task_mexa:.2f}x+{b_xxtoen_task_mexa:.2f}')\n",
    "        axes[1].text(0.3, 0.7, f'Pearson r: {corr_xxtoen_task_mexa:.2f}', fontsize=12, color='red')\n",
    "        axes[1].set_xlabel('MEXA Task')\n",
    "        axes[1].set_ylabel('XX->En Premise COMET')\n",
    "        axes[1].set_title(f'MEXA Task vs XX->En Premise')\n",
    "        axes[1].set_xlim(0,1)\n",
    "        axes[1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Pearson Correlation: MEXA (Task) vs Translation Quality in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/correl_mexa_vs_translationquality')\n",
    "        plt.show()\n",
    "    \n",
    "    if mode == \"alignment-accuracy\":\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        corr_dali_acc, _ = pearsonr(dali, accuracies)\n",
    "        corr_dalistrong_acc, _ = pearsonr(dali_strong, accuracies)\n",
    "        corr_mexaflores_acc, _ = pearsonr(mexa_flores, accuracies)\n",
    "        corr_mexadataset_acc, _ = pearsonr(mexa_dataset, accuracies)\n",
    "\n",
    "        slope_dali_acc, b_dali_acc = np.polyfit(dali, accuracies, 1)\n",
    "        slope_dalistrong_acc, b_dalistrong_acc = np.polyfit(dali_strong, accuracies, 1)\n",
    "        slope_mexaflores_acc, b_mexaflores_acc = np.polyfit(mexa_flores, accuracies, 1)\n",
    "        slope_mexadataset_acc, b_mexadataset_acc = np.polyfit(mexa_dataset, accuracies, 1)\n",
    "\n",
    "        axes[0][0].scatter(dali, accuracies, alpha=0.7, color='blue')\n",
    "        axes[0][0].plot(dali, np.array(dali) * slope_dali_acc + b_dali_acc, color='red', label=f'Linear fit: y={slope_dali_acc:.2f}x+{b_dali_acc:.2f}')\n",
    "        axes[0][0].text(0.3, 0.7, f'Spearman r: {corr_dali_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][0].set_xlabel('DALI (Maxpool)')\n",
    "        axes[0][0].set_ylabel('Accuracy')\n",
    "        axes[0][0].set_title(f'DALI vs Accuracy')\n",
    "        axes[0][0].set_xlim(0,1)\n",
    "        axes[0][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][0].grid(True)\n",
    "\n",
    "        axes[0][1].scatter(dali_strong, accuracies, alpha=0.7, color='blue')\n",
    "        axes[0][1].plot(dali_strong, np.array(dali_strong) * slope_dalistrong_acc + b_dalistrong_acc, color='red', label=f'Linear fit: y={slope_dalistrong_acc:.2f}x+{b_dalistrong_acc:.2f}')\n",
    "        axes[0][1].text(0.3, 0.7, f'Spearman r: {corr_dalistrong_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[0][1].set_xlabel('DALIStrong (Maxpool)')\n",
    "        axes[0][1].set_ylabel('Accuracy')\n",
    "        axes[0][1].set_title(f'DALIStrong vs Accuracy')\n",
    "        axes[0][1].set_xlim(0,1)\n",
    "        axes[0][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[0][1].grid(True)\n",
    "\n",
    "        axes[1][0].scatter(mexa_flores, accuracies, alpha=0.7, color='blue')\n",
    "        axes[1][0].plot(mexa_flores, np.array(mexa_flores) * slope_mexaflores_acc + b_mexaflores_acc, color='red', label=f'Linear fit: y={slope_mexaflores_acc:.2f}x+{b_mexaflores_acc:.2f}')\n",
    "        axes[1][0].text(0.3, 0.7, f'Spearman r: {corr_mexaflores_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][0].set_xlabel('MEXA Flores (Maxpool)')\n",
    "        axes[1][0].set_ylabel('Accuracy')\n",
    "        axes[1][0].set_title(f'MEXA Flores vs Accuracy')\n",
    "        axes[1][0].set_xlim(0,1)\n",
    "        axes[1][0].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][0].grid(True)\n",
    "\n",
    "        axes[1][1].scatter(mexa_dataset, accuracies, alpha=0.7, color='blue')\n",
    "        axes[1][1].plot(mexa_dataset, np.array(mexa_dataset) * slope_mexadataset_acc + b_mexadataset_acc, color='red', label=f'Linear fit: y={slope_mexadataset_acc:.2f}x+{b_mexadataset_acc:.2f}')\n",
    "        axes[1][1].text(0.3, 0.7, f'Spearman r: {corr_mexadataset_acc:.2f}', fontsize=12, color='red')\n",
    "        axes[1][1].set_xlabel('MEXA Dataset (Maxpool)')\n",
    "        axes[1][1].set_ylabel('Accuracy')\n",
    "        axes[1][1].set_title(f'MEXA Dataset vs Accuracy')\n",
    "        axes[1][1].set_xlim(0,1)\n",
    "        axes[1][1].set_ylim(0,1)\n",
    "        #axes[0].legend()\n",
    "        axes[1][1].grid(True)\n",
    "    \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.suptitle('Pearson Correlation: Cross-Lingual Alignment vs Accuracy in Xstorycloze')\n",
    "        \n",
    "        plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/correl_alignment_vs_accuracy')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pearson_correlation(entoxx_dataset_agg, \n",
    "                        xxtoen_dataset_agg, \n",
    "                        entoxx_flores_agg, \n",
    "                        xxtoen_flores_agg, \n",
    "                        entoxx_premise_agg,\n",
    "                        xxtoen_premise_agg,\n",
    "                        accuracies, \n",
    "                        mean_pooled_dali_scores, \n",
    "                        mean_pooled_dalistrong_scores,\n",
    "                        mean_pooled_mexaflores_scores,\n",
    "                        mean_pooled_mexadataset_scores,\n",
    "                        mode='alignment-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies for each language\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "accuracies = {lang: (sum(acc_dict[lang])/len(acc_dict[lang])) * 100 for lang in LANGUAGE}\n",
    "\n",
    "# Create a figure with subplots (2 rows, 5 columns)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (lang, ax) in enumerate(zip(LANGUAGE, axes)):\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    # Get accuracy data for this language\n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_correct_lang_ids = list(set(incorrect_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_incorrect_lang_ids = list(set(incorrect_id_eng) & set(incorrect_id_lang))\n",
    "\n",
    "    \n",
    "    # Get DAS data for this language\n",
    "    lang_DAS = plot_DALI('xstorycloze', lang, 'Llama3.1', mode='DALI')\n",
    "    lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                for outer_k, inner_v in lang_DAS.items()}\n",
    "                \n",
    "\n",
    "    # Calculate averages for both cases\n",
    "    all_list = defaultdict(list)\n",
    "    cc_list = defaultdict(list)\n",
    "    cw_list = defaultdict(list)\n",
    "    wc_list = defaultdict(list)\n",
    "    ww_list = defaultdict(list)\n",
    "\n",
    "    for item in range(1511):\n",
    "        for layer in range(32):\n",
    "            all_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in correct_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cc_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in correct_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cw_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            wc_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            ww_list[layer].append(lang_DAS[item][layer])\n",
    "    \n",
    "        \n",
    "    all_mean = []\n",
    "    cc_mean = []\n",
    "    cw_mean = []\n",
    "    ww_mean = []\n",
    "    wc_mean = []\n",
    "\n",
    "    for k,v in all_list.items():\n",
    "        all_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cc_list.items():\n",
    "        cc_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cw_list.items():\n",
    "        cw_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in ww_list.items():\n",
    "        ww_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in wc_list.items():\n",
    "        wc_mean.append(np.mean(v))\n",
    "\n",
    "    \n",
    "    max_idx = np.argmax(all_mean)\n",
    "    count = [np.sum(cc_list[max_idx]), np.sum(cw_list[max_idx])]\n",
    "    nobs = [len(cc_list[max_idx]), len(cw_list[max_idx])]\n",
    "    stat, pval = smp.proportions_ztest(count, nobs, alternative=\"larger\")\n",
    "    annotation_text = f'Δ: {cc_mean[max_idx]-cw_mean[max_idx]:.3f}\\np-value: {pval:.3f}'\n",
    "    ax.annotate(annotation_text, xy=(0.5, 0.8), xycoords='axes fraction', fontsize=12,\n",
    "                bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "    # Plot lines for this language\n",
    "    layers = list(range(32))\n",
    "    ax.plot(layers, [cc_mean[l] for l in layers], \n",
    "            label='EC-XC', marker='o', color='green', markersize=1)\n",
    "    ax.plot(layers, [cw_mean[l] for l in layers], \n",
    "            label='EC-XW', color = 'red', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [ww_mean[l] for l in layers], \n",
    "    #        label='WW', color = 'teal', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [wc_mean[l] for l in layers], \n",
    "    #        label='WC', color = 'magenta', marker='o', markersize=1)\n",
    "    \n",
    "    \n",
    "    ax.set_ylim(0,1)\n",
    "   \n",
    "    \n",
    "    # Add title with accuracy\n",
    "    ax.set_title(f'{lang.capitalize()}', size=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    #ax.axvspan(8, 20, color='lightblue', alpha=0.5)\n",
    "    \n",
    "    # Only add x and y labels for the bottom row and leftmost column\n",
    "    if idx >= 5:  # Bottom row\n",
    "        ax.set_xlabel('Layer', size=14)\n",
    "     \n",
    "    if idx % 5 == 0:  # Leftmost column\n",
    "        ax.set_ylabel('% DALI', size=14)\n",
    "       \n",
    "    \n",
    "    # Add legend only for the first subplot\n",
    "    if idx == 0:\n",
    "        ax.legend(bbox_to_anchor=(0.99, 0.01), loc='lower right')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/DALItrajectory_cohort')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies for each language\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "accuracies = {lang: (sum(acc_dict[lang])/len(acc_dict[lang])) * 100 for lang in LANGUAGE}\n",
    "\n",
    "# Create a figure with subplots (2 rows, 5 columns)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (lang, ax) in enumerate(zip(LANGUAGE, axes)):\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    # Get accuracy data for this language\n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_correct_lang_ids = list(set(incorrect_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_incorrect_lang_ids = list(set(incorrect_id_eng) & set(incorrect_id_lang))\n",
    "\n",
    "    \n",
    "    # Get DAS data for this language\n",
    "    lang_DAS = plot_DALI('xstorycloze', lang, 'Llama3.1', mode='DALIStrong')\n",
    "    lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                for outer_k, inner_v in lang_DAS.items()}\n",
    "                \n",
    "\n",
    "    # Calculate averages for both cases\n",
    "    all_list = defaultdict(list)\n",
    "    cc_list = defaultdict(list)\n",
    "    cw_list = defaultdict(list)\n",
    "    wc_list = defaultdict(list)\n",
    "    ww_list = defaultdict(list)\n",
    "\n",
    "    for item in range(1511):\n",
    "        for layer in range(32):\n",
    "            all_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in correct_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cc_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in correct_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cw_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            wc_list[layer].append(lang_DAS[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            ww_list[layer].append(lang_DAS[item][layer])\n",
    "    \n",
    "        \n",
    "    all_mean = []\n",
    "    cc_mean = []\n",
    "    cw_mean = []\n",
    "    ww_mean = []\n",
    "    wc_mean = []\n",
    "\n",
    "    for k,v in all_list.items():\n",
    "        all_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cc_list.items():\n",
    "        cc_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cw_list.items():\n",
    "        cw_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in ww_list.items():\n",
    "        ww_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in wc_list.items():\n",
    "        wc_mean.append(np.mean(v))\n",
    "\n",
    "    \n",
    "    max_idx = np.argmax(all_mean)\n",
    "    count = [np.sum(cc_list[max_idx]), np.sum(cw_list[max_idx])]\n",
    "    nobs = [len(cc_list[max_idx]), len(cw_list[max_idx])]\n",
    "    stat, pval = smp.proportions_ztest(count, nobs, alternative=\"larger\")\n",
    "    annotation_text = f'Δ: {cc_mean[max_idx]-cw_mean[max_idx]:.3f}\\np-value: {pval:.3f}'\n",
    "    ax.annotate(annotation_text, xy=(0.5, 0.8), xycoords='axes fraction', fontsize=12,\n",
    "                bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "    # Plot lines for this language\n",
    "    layers = list(range(32))\n",
    "    ax.plot(layers, [cc_mean[l] for l in layers], \n",
    "            label='EC-XC', marker='o', color='green', markersize=1)\n",
    "    ax.plot(layers, [cw_mean[l] for l in layers], \n",
    "            label='EC-XW', color = 'red', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [ww_mean[l] for l in layers], \n",
    "    #        label='WW', color = 'teal', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [wc_mean[l] for l in layers], \n",
    "    #        label='WC', color = 'magenta', marker='o', markersize=1)\n",
    "    \n",
    "    \n",
    "    ax.set_ylim(0,1)\n",
    "   \n",
    "    \n",
    "    # Add title with accuracy\n",
    "    ax.set_title(f'{lang.capitalize()}', size=16)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    #ax.axvspan(8, 20, color='lightblue', alpha=0.5)\n",
    "    \n",
    "    # Only add x and y labels for the bottom row and leftmost column\n",
    "    if idx >= 5:  # Bottom row\n",
    "        ax.set_xlabel('Layer', size=14)\n",
    "     \n",
    "    if idx % 5 == 0:  # Leftmost column\n",
    "        ax.set_ylabel('% DALI.S', size=14)\n",
    "       \n",
    "    \n",
    "    # Add legend only for the first subplot\n",
    "    if idx == 0:\n",
    "        ax.legend(bbox_to_anchor=(0.99, 0.01), loc='lower right')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/DALIStrongtrajectory_cohort')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies for each language\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "accuracies = {lang: (sum(acc_dict[lang])/len(acc_dict[lang])) * 100 for lang in LANGUAGE}\n",
    "\n",
    "# Create a figure with subplots (2 rows, 5 columns)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (lang, ax) in enumerate(zip(LANGUAGE, axes)):\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    # Get accuracy data for this language\n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_correct_lang_ids = list(set(incorrect_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_incorrect_lang_ids = list(set(incorrect_id_eng) & set(incorrect_id_lang))\n",
    "\n",
    "    \n",
    "    # Get DAS data for this language\n",
    "    lang_DAS = plot_DALI('xstorycloze', lang, 'Llama3.1', mode='MEXATask')\n",
    "    lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "    lang_DAS_formatted = defaultdict(dict)\n",
    "    for layer in range(32):\n",
    "        for i,dali in enumerate(lang_DAS[layer]):\n",
    "            lang_DAS_formatted[i][layer] = lang_DAS[layer][i]\n",
    "\n",
    "    \n",
    "    # Calculate averages for both cases\n",
    "    all_list = defaultdict(list)\n",
    "    cc_list = defaultdict(list)\n",
    "    cw_list = defaultdict(list)\n",
    "    wc_list = defaultdict(list)\n",
    "    ww_list = defaultdict(list)\n",
    "\n",
    "    for item in range(1511):\n",
    "        for layer in range(32):\n",
    "            all_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "\n",
    "    for item in correct_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cc_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "\n",
    "    for item in correct_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            cw_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_correct_lang_ids:\n",
    "        for layer in range(32):\n",
    "            wc_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "\n",
    "    for item in incorrect_eng_incorrect_lang_ids:\n",
    "        for layer in range(32):\n",
    "            ww_list[layer].append(lang_DAS_formatted[item][layer])\n",
    "    \n",
    "        \n",
    "    all_mean = []\n",
    "    cc_mean = []\n",
    "    cw_mean = []\n",
    "    ww_mean = []\n",
    "    wc_mean = []\n",
    "\n",
    "    for k,v in all_list.items():\n",
    "        all_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cc_list.items():\n",
    "        cc_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in cw_list.items():\n",
    "        cw_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in ww_list.items():\n",
    "        ww_mean.append(np.mean(v))\n",
    "\n",
    "    for k,v in wc_list.items():\n",
    "        wc_mean.append(np.mean(v))\n",
    "\n",
    "    \n",
    "    max_idx = np.argmax(all_mean)\n",
    "    \n",
    "    \n",
    "    count = [np.sum(cc_list[max_idx]), np.sum(cw_list[max_idx])]\n",
    "    nobs = [len(cc_list[max_idx]), len(cw_list[max_idx])]\n",
    "    stat, pval = smp.proportions_ztest(count, nobs, alternative=\"larger\")\n",
    "    annotation_text = f'Δ: {cc_mean[max_idx]-cw_mean[max_idx]:.3f}\\np-value: {pval:.3f}'\n",
    "    ax.annotate(annotation_text, xy=(0.5, 0.8), xycoords='axes fraction', fontsize=12,\n",
    "                bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "    # Plot lines for this language\n",
    "    layers = list(range(32))\n",
    "    ax.plot(layers, [cc_mean[l] for l in layers], \n",
    "            label='EC-XC', marker='o', color='green', markersize=1)\n",
    "    ax.plot(layers, [cw_mean[l] for l in layers], \n",
    "            label='EC-XW', color = 'red', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [ww_mean[l] for l in layers], \n",
    "    #        label='WW', color = 'teal', marker='o', markersize=1)\n",
    "    #ax.plot(layers, [wc_mean[l] for l in layers], \n",
    "    #        label='WC', color = 'magenta', marker='o', markersize=1)\n",
    "    \n",
    "    \n",
    "    ax.set_ylim(0,1)\n",
    "    \n",
    "    # Add title with accuracy\n",
    "    ax.set_title(f'{lang.capitalize()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    #ax.axvspan(8, 20, color='lightblue', alpha=0.5)\n",
    "    \n",
    "    if idx >= 5:  # Bottom row\n",
    "        ax.set_xlabel('Layer', size=14)\n",
    "        \n",
    "    if idx % 5 == 0:  # Leftmost column\n",
    "        ax.set_ylabel('% MEXA.T', size=14)\n",
    "        \n",
    "        \n",
    "        # Add legend only for the first subplot\n",
    "    if idx == 0:\n",
    "        ax.legend(bbox_to_anchor=(0.99, 0.01), loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname='../../../../Images_DALI/xstorycloze_plots/MEXATasktrajectory_cohort')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fields_list = ['input_sentence_1', 'input_sentence_2', 'input_sentence_3', 'input_sentence_4', 'sentence_quiz1', 'sentence_quiz2']\n",
    "entoxx_sample = defaultdict(dict)\n",
    "xxtoen_sample = defaultdict(dict)\n",
    "\n",
    "for field in input_fields_list:\n",
    "    entoxx, xxtoen = load_translation_sample('xstorycloze', 'Llama3.1', field)\n",
    "    for lang in LANGUAGE:\n",
    "        entoxx_sample[field][lang] = entoxx[lang]['scores']\n",
    "        xxtoen_sample[field][lang] = xxtoen[lang]['scores']\n",
    "\n",
    "\n",
    "\n",
    "entoxx_mean_diff = defaultdict(dict)\n",
    "xxtoen_mean_diff = defaultdict(dict)\n",
    "\n",
    "for idx, (lang, ax) in enumerate(zip(LANGUAGE, axes)):\n",
    "    # Get accuracy data for this language\n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_correct_lang_ids = list(set(incorrect_id_eng) & set(correct_id_lang))\n",
    "    incorrect_eng_incorrect_lang_ids = list(set(incorrect_id_eng) & set(incorrect_id_lang))\n",
    "\n",
    "    for field in input_fields_list:\n",
    "        entoxx_mean_diff[lang][field] = np.mean([entoxx_sample[field][lang][i] for i in correct_eng_correct_lang_ids]) - np.mean([entoxx_sample[field][lang][i] for i in correct_eng_incorrect_lang_ids]) \n",
    "        xxtoen_mean_diff[lang][field] = np.mean([xxtoen_sample[field][lang][i] for i in correct_eng_correct_lang_ids]) - np.mean([xxtoen_sample[field][lang][i] for i in correct_eng_incorrect_lang_ids]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(entoxx_mean_diff, orient='index')\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the heatmap\n",
    "ax = sns.heatmap(df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".3f\", \n",
    "                linewidths=.5, cbar_kws={\"shrink\": .8, \"label\": \"Δ COMET scores (En-XX)\"})\n",
    "\n",
    "# Add labels and title\n",
    "#plt.title('Language Features Heatmap', fontsize=16)\n",
    "#plt.ylabel('Language', fontsize=14)\n",
    "plt.xlabel('Input field')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(xxtoen_mean_diff, orient='index')\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the heatmap\n",
    "ax = sns.heatmap(df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".3f\", \n",
    "                linewidths=.5, cbar_kws={\"shrink\": .8, \"label\": \"Δ COMET scores (XX-En)\"})\n",
    "\n",
    "# Add labels and title\n",
    "#plt.title('Language Features Heatmap', fontsize=16)\n",
    "#plt.ylabel('Language', fontsize=14)\n",
    "plt.xlabel('Input field')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
