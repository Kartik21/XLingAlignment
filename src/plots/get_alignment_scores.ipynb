{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset #load_dataset from Huggingface\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata, spearmanr, pearsonr\n",
    "import statsmodels.stats.proportion as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.format\"] = 'pdf'\n",
    "plt.rcParams['font.family'] = 'Palatino'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_DICT = {'afrikaans':'afr_Latn' ,\n",
    "'english': 'eng_Latn',\n",
    "'amharic':'amh_Ethi' ,\n",
    "'armenian':'hye_Armn' ,\n",
    "'assamese':'asm_Beng' ,\n",
    "'basque':'eus_Latn' ,\n",
    "'bengali':'ben_Beng' ,\n",
    "'bulgarian':'bul_Cyrl' ,\n",
    "'burmese':'mya_Mymr' ,\n",
    "'catalan':'cat_Latn' ,\n",
    "'central kurdish':'ckb_Arab' ,\n",
    "'croatian': 'hrv_Latn',\n",
    "'dutch': 'nld_Latn',\n",
    "'xhosa': 'xho_Latn',\n",
    "'macedonian': 'mkd_Cyrl',\n",
    "'czech':'ces_Latn' ,\n",
    "'danish':'dan_Latn' ,\n",
    "'eastern panjabi':'pan_Guru' ,\n",
    "'egyptian arabic':'arz_Arab' ,\n",
    "'estonian':'est_Latn' ,\n",
    "'finnish':'fin_Latn' ,\n",
    "'french':'fra_Latn' ,\n",
    "'georgian':'kat_Geor' ,\n",
    "'german':'deu_Latn' ,\n",
    "'greek':'ell_Grek' ,\n",
    "'gujarati':'guj_Gujr' ,\n",
    "'hausa':'hau_Latn' ,\n",
    "'hebrew':'heb_Hebr' ,\n",
    "'hindi':'hin_Deva' ,\n",
    "'hungarian':'hun_Latn' ,\n",
    "'icelandic':'isl_Latn' ,\n",
    "'indonesian':'ind_Latn' ,\n",
    "'italian':'ita_Latn' ,\n",
    "'japanese':'jpn_Jpan' ,\n",
    "'javanese':'jav_Latn' ,\n",
    "'kannada':'kan_Knda' ,\n",
    "'kazakh':'kaz_Cyrl' ,\n",
    "'khmer':'khm_Khmr' ,\n",
    "'korean':'kor_Hang' ,\n",
    "'kyrgyz':'kir_Cyrl' ,\n",
    "'lao':'lao_Laoo' ,\n",
    "'lithuanian':'lit_Latn' ,\n",
    "'malayalam':'mal_Mlym' ,\n",
    "'marathi':'mar_Deva' ,\n",
    "'mesopotamian arabic':'acm_Arab' ,\n",
    "'modern standard arabic':'arb_Arab' ,\n",
    "'moroccan arabic':'ary_arab' ,\n",
    "'najdi arabic':'ars_Arab' ,\n",
    "'nepali':'npi_Deva' ,\n",
    "'north azerbaijani':'azj_Latn' ,\n",
    "'north levantine arabic':'apc_Arab' ,\n",
    "'northern uzbek':'uzn_Latn' ,\n",
    "'norwegian bokmal':'nob_Latn' ,\n",
    "'odia':'ory_Orya' ,\n",
    "'polish':'pol_Latn' ,\n",
    "'portuguese':'por_Latn' ,\n",
    "'romanian':'ron_Latn' ,\n",
    "'russian':'rus_Cyrl' ,\n",
    "'serbian':'srp_Cyrl' ,\n",
    "'simplified chinese':'zho_Hans' ,\n",
    "'sindhi':'snd_Arab' ,\n",
    "'sinhala':'sin_Sinh' ,\n",
    "'slovak':'slk_Latn' ,\n",
    "'slovenian':'slv_Latn' ,\n",
    "'somali':'som_Latn' ,\n",
    "'southern pashto':'pbt_Arab' ,\n",
    "'spanish':'spa_Latn' ,\n",
    "'standard latvian':'lvs_Latn' ,\n",
    "'standard malay':'zsm_Latn' ,\n",
    "'sundanese':'sun_Latn' ,\n",
    "'swahili':'swh_Latn' ,\n",
    "'swedish':'swe_Latn' ,\n",
    "'tamil':'tam_Taml' ,\n",
    "'telugu':'tel_Telu' ,\n",
    "'thai':'tha_Thai' ,\n",
    "'tosk albanian':'als_Latn' ,\n",
    "'traditional chinese':'zho_Hant' ,\n",
    "'turkish':'tur_Latn' ,\n",
    "'ukrainian':'ukr_Cyrl' ,\n",
    "'urdu':'urd_Arab' ,\n",
    "'vietnamese':'vie_Latn' ,\n",
    "'western persian':'pes_Arab'}\n",
    "\n",
    "LANGUAGE=[k for k,v in LANG_DICT.items()]\n",
    "LANGUAGE_wo_ENGLISH = [k for k,v in LANG_DICT.items() if k!='english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_outputs(lang, dataset='belebele', model='Llama3.1'):\n",
    "    model_dict = {'Llama3.1': 'meta-llama__Llama-3.1-8B'}\n",
    "    lang_code = LANG_DICT[lang]\n",
    "    model_code = model_dict[model]\n",
    "    #hf = load_dataset(\"Kartik221/Belebele_test\", lang_code)\n",
    "    #label = hf['test']['correct_answer_num']\n",
    "    #answer = []\n",
    "    #for i in range(len(label)):\n",
    "        #answer.append(label[i])\n",
    "\n",
    "    accuracy_data_path = f'../../accuracy_outputs/{model}/{dataset}_5shot/{lang_code}/'\n",
    "\n",
    "    # Find the .jsonl file in the directory\n",
    "    jsonl_file = [f for f in os.listdir(accuracy_data_path) if f.endswith('.jsonl')][0]\n",
    "    file_path = os.path.join(accuracy_data_path, jsonl_file)\n",
    "    # Read the jsonl file line by line\n",
    "    accuracy_results = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            accuracy_results.append(json.loads(line))\n",
    "\n",
    "    resps = [item['resps'] for item in accuracy_results]\n",
    "    accuracy = [item['acc'] for item in accuracy_results]\n",
    "\n",
    "    score_diff = []\n",
    "    for i in range(len(accuracy)):\n",
    "        logprob = [float(resps[i][0][0][0]), float(resps[i][1][0][0]), float(resps[i][2][0][0]), float(resps[i][3][0][0])]\n",
    "        model_pred_idx = np.argmax(logprob)\n",
    "        pred_logprob = logprob[model_pred_idx]\n",
    "        next_best_logprob = max([logprob[i] for i in range(3) if i!=model_pred_idx])    \n",
    "        score_diff.append(pred_logprob - next_best_logprob)\n",
    "    return score_diff, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = defaultdict(dict)\n",
    "score_diff_dict = defaultdict(dict)\n",
    "\n",
    "conf_matrices = defaultdict(dict)\n",
    "\n",
    "score_diff_dict['english'], acc_dict['english'] = get_accuracy_outputs('english', 'belebele', 'Llama3.1')\n",
    "\n",
    "for lang in LANGUAGE:\n",
    "    score_diff_dict[lang],acc_dict[lang]  = get_accuracy_outputs(lang, 'belebele', 'Llama3.1')\n",
    "    ''' \n",
    "    correct_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==1]\n",
    "    incorrect_id_eng = [i for i,acc in enumerate(acc_dict['english']) if acc==0]\n",
    "       \n",
    "    correct_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==1]\n",
    "    incorrect_id_lang = [i for i,acc in enumerate(acc_dict[lang]) if acc==0]\n",
    "\n",
    "    # Example language data (Replace with actual lists)\n",
    "    conf_matrices[lang] = compute_confusion_matrix(correct_id_eng, incorrect_id_eng, correct_id_lang, incorrect_id_lang)\n",
    "    \n",
    "    correct_eng_incorrect_lang_ids = list(set(correct_id_eng) & set(incorrect_id_lang))\n",
    "    correct_eng_correct_lang_ids = list(set(correct_id_eng) & set(correct_id_lang))\n",
    "\n",
    "# Plot multiple confusion matrices\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))  # 2 rows, 5 columns\n",
    "\n",
    "for ax, (lang, matrix) in zip(axes.flat, conf_matrices.items()):\n",
    "    \n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Correct_Eng\", \"Incorrect_Eng\"], \n",
    "                yticklabels=[\"Correct_Lang\", \"Incorrect_Lang\"], ax=ax)\n",
    "    ax.set_title(lang)\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.show()   \n",
    "    \n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DALI(dataset, lang, model, mode):\n",
    "    if dataset == 'belebele':\n",
    "        lang_code = LANG_DICT[lang]\n",
    "    \n",
    "        \n",
    "    if mode == 'DALI':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali/DALI_{lang_code}_lasttoken.json'\n",
    "    if mode == 'DALIStrong':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali_strong/DALI_{lang_code}_lasttoken.json'\n",
    "\n",
    "    if mode == 'MEXAFlores':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/flores_mexa/{lang_code}.json'\n",
    "    if mode == 'MEXATask':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang_code}.json'\n",
    "    with open(DAS_path) as f:\n",
    "        lang_DAS = json.load(f)\n",
    "    return lang_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies and get DAS scores for each language\n",
    "def plot_alignment_by_layers(mode):\n",
    "    accuracies = {}\n",
    "    max_das_scores = {}\n",
    "    mean_das_scores={}\n",
    "    all_das_avgs = {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.rcParams['font.family'] = 'Palatino'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for lang in LANGUAGE_wo_ENGLISH:\n",
    "        # Calculate accuracy\n",
    "        #accuracies[lang] = (sum(acc_dict[lang])/len(acc_dict[lang])) \n",
    "        # Get DAS data for this language\n",
    "        lang_DAS = plot_DALI('belebele', lang, 'Llama3.1', mode)\n",
    "        \n",
    "        if mode == 'DALI' or mode == 'DALIStrong':\n",
    "            lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                        for outer_k, inner_v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_scores = [lang_DAS[sample][layer] for sample in lang_DAS.keys()]\n",
    "                layer_avgs.append(np.mean(layer_scores))\n",
    "\n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "\n",
    "            \n",
    "        else: \n",
    "            lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_avgs.append(np.mean(lang_DAS[layer]))      \n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            \n",
    "\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.tick_params(axis='both',labelsize=14)\n",
    "    ax.set_ylabel(f'{mode}')\n",
    "    #ax.set_title('DALI Scores Across Layers (story completion)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_ylim(0,1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return accuracies, max_das_scores, mean_das_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, max_dali, mean_dali = plot_alignment_by_layers('DALI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, max_dali_strong, mean_dali_strong = plot_alignment_by_layers('DALIStrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, max_mexa_t, mean_mexa_t = plot_alignment_by_layers('MEXATask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, max_mexa_flores, mean_mexa_flores = plot_alignment_by_layers('MEXAFlores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the required metrics\n",
    "data = {\n",
    "    'Language': LANGUAGE_wo_ENGLISH,\n",
    "    'Max_DALI': [max_dali[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Mean_DALI': [mean_dali[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Max_DALI_Strong': [max_dali_strong[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Mean_DALI_Strong': [mean_dali_strong[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Max_MEXA_T': [max_mexa_t[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Mean_MEXA_T': [mean_mexa_t[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Max_MEXA_Flores': [max_mexa_flores[lang] for lang in LANGUAGE_wo_ENGLISH],\n",
    "    'Mean_MEXA_Flores': [mean_mexa_flores[lang] for lang in LANGUAGE_wo_ENGLISH]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_excel(\"../../../../Images_DALI/belebele_pooled_alignment.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DALI_xstorycloze(dataset, lang, model, mode):\n",
    "    if mode == 'DALI':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali/BAS_{lang}_lasttoken.json'\n",
    "    if mode == 'DALIStrong':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali_strong/DALI_{lang}_lasttoken.json'\n",
    "\n",
    "    if mode == 'MEXAFlores':\n",
    "        flores_dict = {'arabic': 'arb_Arab', \n",
    "                       'spanish': 'spa_Latn',\n",
    "                        'basque': 'eus_Latn',\n",
    "                         'hindi': 'hin_Deva',\n",
    "                          'indonesian': 'ind_Latn',\n",
    "                           'burmese': 'mya_Mymr',\n",
    "                            'russian': 'rus_Cyrl',\n",
    "                             'telugu': 'tel_Telu',\n",
    "                              'chinese': 'zho_Hans',\n",
    "                               'swahili': 'swh_Latn'}\n",
    "        \n",
    "        lang_code = flores_dict[lang]\n",
    "\n",
    "        DAS_path = f'../../alignment_outputs/{model}/flores_mexa/{lang_code}.json'\n",
    "    if mode == 'MEXATask':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang}.json'\n",
    "    with open(DAS_path) as f:\n",
    "        lang_DAS = json.load(f)\n",
    "    return lang_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies and get DAS scores for each language\n",
    "def plot_alignment_by_layers_xstorycloze(mode):\n",
    "    accuracies = {}\n",
    "    max_das_scores = {}\n",
    "    mean_das_scores = {}\n",
    "    all_das_avgs = {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.rcParams['font.family'] = 'Palatino'\n",
    "\n",
    "    lang_list = ['arabic', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'chinese', 'swahili']\n",
    "    for lang in lang_list:\n",
    "        # Calculate accuracy\n",
    "        #accuracies[lang] = (sum(acc_dict[lang])/len(acc_dict[lang])) \n",
    "        # Get DAS data for this language\n",
    "        lang_DAS = plot_DALI_xstorycloze('xstorycloze', lang, 'Llama3.1', mode)\n",
    "        \n",
    "        if mode == 'DALI' or mode == 'DALIStrong':\n",
    "            lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                        for outer_k, inner_v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_scores = [lang_DAS[sample][layer] for sample in lang_DAS.keys()]\n",
    "                layer_avgs.append(np.mean(layer_scores))\n",
    "\n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "\n",
    "            \n",
    "        else: \n",
    "            lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_avgs.append(np.mean(lang_DAS[layer]))      \n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            \n",
    "\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.tick_params(axis='both',labelsize=14)\n",
    "    ax.set_ylabel(f'{mode}')\n",
    "    #ax.set_title('DALI Scores Across Layers (story completion)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_ylim(0,1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return max_das_scores, mean_das_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dali_story, mean_dali_story = plot_alignment_by_layers_xstorycloze('DALI')\n",
    "max_dalistrong_story, mean_dalistrong_story = plot_alignment_by_layers_xstorycloze('DALIStrong')\n",
    "max_mexa_story, mean_mexa_story = plot_alignment_by_layers_xstorycloze('MEXATask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the required metrics\n",
    "lang_list= ['arabic', 'spanish', 'basque', 'hindi', 'indonesian', 'burmese', 'russian', 'telugu', 'chinese', 'swahili']\n",
    "\n",
    "data = {\n",
    "    'Language': lang_list,\n",
    "    'Max_DALI': [max_dali_story[lang] for lang in lang_list],\n",
    "    'Mean_DALI': [mean_dali_story[lang] for lang in lang_list],\n",
    "    'Max_DALI_Strong': [max_dalistrong_story[lang] for lang in lang_list],\n",
    "    'Mean_DALI_Strong': [mean_dalistrong_story[lang] for lang in lang_list],\n",
    "    'Max_MEXA_T': [max_mexa_story[lang] for lang in lang_list],\n",
    "    'Mean_MEXA_T': [mean_mexa_story[lang] for lang in lang_list]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_excel(\"../../../../Images_DALI/xstorycloze_pooled_alignment.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DALI_xcopa(dataset, lang, model, mode):\n",
    "    if mode == 'DALI':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali/DALI_{lang}_lasttoken.json'\n",
    "    if mode == 'DALIStrong':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_dali_strong/DALI_{lang}_lasttoken.json'\n",
    "\n",
    "    if mode == 'MEXAFlores':\n",
    "        flores_dict = {'chinese': 'zho_Hans', \n",
    "                       'indonesian': 'ind_Latn',\n",
    "                        'italian': 'ita_Latn',\n",
    "                         'swahili': 'swh_Latn',\n",
    "                          'tamil': 'tam_Taml',\n",
    "                           'thai': 'tha_Thai',\n",
    "                            'turkish': 'tur_Latn',\n",
    "                             'vietnamese': 'vie_Latn'}\n",
    "        \n",
    "        lang_code = flores_dict[lang]\n",
    "\n",
    "        DAS_path = f'../../alignment_outputs/{model}/flores_mexa/{lang_code}.json'\n",
    "    if mode == 'MEXATask':\n",
    "        DAS_path = f'../../alignment_outputs/{model}/{dataset}_mexa/{lang}.json'\n",
    "    with open(DAS_path) as f:\n",
    "        lang_DAS = json.load(f)\n",
    "    return lang_DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies and get DAS scores for each language\n",
    "def plot_alignment_by_layers_xcopa(mode):\n",
    "    accuracies = {}\n",
    "    max_das_scores = {}\n",
    "    all_das_avgs = {}\n",
    "    mean_das_scores={}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.rcParams['font.family'] = 'Palatino'\n",
    "\n",
    "    lang_list = ['chinese', 'indonesian', 'italian', 'swahili', 'tamil', 'thai', 'turkish', 'vietnamese']\n",
    "\n",
    "    for lang in lang_list:\n",
    "        # Calculate accuracy\n",
    "        #accuracies[lang] = (sum(acc_dict[lang])/len(acc_dict[lang])) \n",
    "        # Get DAS data for this language\n",
    "        lang_DAS = plot_DALI_xcopa('xcopa', lang, 'Llama3.1', mode)\n",
    "        \n",
    "        if mode == 'DALI' or mode == 'DALIStrong':\n",
    "            lang_DAS = {int(outer_k): {int(inner_k): v for inner_k, v in inner_v.items()} \n",
    "                        for outer_k, inner_v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_scores = [lang_DAS[sample][layer] for sample in lang_DAS.keys()]\n",
    "                layer_avgs.append(np.mean(layer_scores))\n",
    "\n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "\n",
    "            \n",
    "        else: \n",
    "            lang_DAS = {int(k): v for k,v in lang_DAS.items()}\n",
    "            layer_avgs = []\n",
    "            for layer in range(32):\n",
    "                layer_avgs.append(np.mean(lang_DAS[layer]))      \n",
    "            all_das_avgs[lang] = layer_avgs\n",
    "            max_das_scores[lang] = np.max(all_das_avgs[lang])\n",
    "            mean_das_scores[lang] = np.mean(all_das_avgs[lang])\n",
    "            \n",
    "\n",
    "            ax.plot(range(32), layer_avgs, \n",
    "                label=f'{lang.capitalize()}', \n",
    "                marker='o', markersize=2)\n",
    "\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.tick_params(axis='both',labelsize=14)\n",
    "    ax.set_ylabel(f'{mode}')\n",
    "    #ax.set_title('DALI Scores Across Layers (story completion)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_ylim(0,1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return max_das_scores, mean_das_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dali_copa, mean_dali_copa = plot_alignment_by_layers_xcopa('DALI')\n",
    "max_dalistrong_copa, mean_dalistrong_copa = plot_alignment_by_layers_xcopa('DALIStrong')\n",
    "max_mexa_copa, mean_mexa_copa = plot_alignment_by_layers_xcopa('MEXATask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the required metrics\n",
    "lang_list= ['chinese', 'indonesian', 'italian', 'swahili', 'tamil', 'thai', 'turkish', 'vietnamese']\n",
    "\n",
    "data = {\n",
    "    'Language': lang_list,\n",
    "    'Max_DALI': [max_dali_copa[lang] for lang in lang_list],\n",
    "    'Mean_DALI': [mean_dali_copa[lang] for lang in lang_list],\n",
    "    'Max_DALI_Strong': [max_dalistrong_copa[lang] for lang in lang_list],\n",
    "    'Mean_DALI_Strong': [mean_dalistrong_copa[lang] for lang in lang_list],\n",
    "    'Max_MEXA_T': [max_mexa_copa[lang] for lang in lang_list],\n",
    "    'Mean_MEXA_T': [mean_mexa_copa[lang] for lang in lang_list]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_excel(\"../../../../Images_DALI/xcopa_pooled_alignment.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
